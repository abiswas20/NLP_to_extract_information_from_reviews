
#we can use the UMAP method we used previously to create embedding in 2d and visualize the findings.
umap_embeddings_2D=umap.UMAP(n_neighbors=100,n_components=2,metric='cosine').fit_transform(embeddings)
df_umap_embeddings_2D=pd.DataFrame(umap_embeddings_2D,columns=['x','y'])
df_umap_embeddings_2D['label']=clusters.labels_

df_umap_embeddings_2D['label'].value_counts(normalize=True)  #the labels are from the 6 dimensional analysis. so value_counts of labels on 2D dataframe is equivalent to value_counts on 6D dataframe.

#Now let's plot df_umap_embeddings_2D

df_umap_embeddings_2D.plot(x='x',y='y',kind='scatter',c='label',cmap='cividis',figsize=(16,12))

plt.savefig('/content/drive/My Drive/amazon_reviews_project/images/clustered_embedding_all_25.png')

#Let's create a column with labels in the original dataframe
data['label']=df_umap_embeddings_2D['label']
data

selected_clusters=[i for i in range(-1,3) if df_umap_embeddings_2D['label'].value_counts(normalize=True)[i]>0.02]
selected_clusters

df_umap_embeddings_2D_selected_clusters=df_umap_embeddings_2D[df_umap_embeddings_2D['label'].isin(selected_clusters)]
df_umap_embeddings_2D_selected_clusters.plot(x='x',y='y',kind='scatter',c='label',cmap='cividis',figsize=(16,12))

plt.savefig('/content/drive/My Drive/amazon_reviews_project/images/clustered_embedding_selected_25.png')

umap_embeddings_3D=umap.UMAP(n_neighbors=100,n_components=3,metric='cosine').fit_transform(embeddings)
df_umap_embeddings_3D=pd.DataFrame(umap_embeddings_3D,columns=['x','y','z'])
df_umap_embeddings_3D['label']=clusters.labels_

df_umap_embeddings_3D_selected_clusters=df_umap_embeddings_3D[df_umap_embeddings_3D['label'].isin(selected_clusters)]

fig = plt.figure(figsize=(15,15))
ax = fig.add_subplot(111, projection='3d')
for cluster in selected_clusters:
    ax.scatter(df_umap_embeddings_3D_selected_clusters.loc[df_umap_embeddings_3D_selected_clusters['label']==cluster]['x'],\
               df_umap_embeddings_3D_selected_clusters.loc[df_umap_embeddings_3D_selected_clusters['label']==cluster]['y'],\
               df_umap_embeddings_3D_selected_clusters.loc[df_umap_embeddings_3D_selected_clusters['label']==cluster]['z'],\
               cmap='cividis',label=cluster)
ax.legend()
plt.tight_layout()
plt.savefig('/content/drive/My Drive/amazon_reviews_project/images/clustered_3D_embedding_selected_25.png')

#We can label the original dataframe and extract reviews corresponding to each label.

data_selected_clusters=data[data['label'].isin(selected_clusters)].copy(deep=True)
data_selected_clusters


def create_doc(input_dataframe,clusters_to_select):
  docs=[]
  for label in clusters_to_select:
    doc=input_dataframe[input_dataframe['label']==label]
    docs.append(doc)
  return docs

docs=create_doc(data_selected_clusters,selected_clusters)
print(docs)

def docs_TFIDF_vectorizer(docs):
  from sklearn.feature_extraction.text import TfidfVectorizer

  stop_words = text.ENGLISH_STOP_WORDS.union(['00', '10', '100', '12', '15', '16', '20', '200', '24', '25',\
       '2nd', '30', '40', '45', '50', '60', '75', '80', '90','!',"''","'m","'s",',','.','...','He','I','It','My','Of','``',\
        '!',"''","'m","'re","'s",',','-','.','...','9','An','Ca','Do','I','It','S.','``','!',"''","'s",'(',')',',','-','.',\
        'b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y','&',"'ll",'-D','5',':','?',\
        "'", '0', '1', '2', '3', '4', '6', '7', '8', 'A', 'C', 'D', 'H', 'M', 'O', 'S', '`','#',"'ve",'*','--','..','....','10/10','4/5',';',\
        'As','At','HE','IS','IT','If','In','MY','No','ON','On','PR','SO','So','St','To','US','We','/', 'E', 'N', 'P', 'R', 'T', 'U', 'W', 'Y',\
        '$','%',"'S","'d",'.....','1/2','1/3','105','12-lead','125','14','150','198','1\\23\\18','22','221','27','3-3.5','35','AM','Be','By',\
        'CK','DJ','De','Dr','HM','JE','K.','L','MB','Mr','Ms','R.','TO','W.','YA','B', 'J', 'K', '\\','@','Im','Me','Is','000','100ish','11',\
        '178','191','1945','1st','2015','2016','260','36','360','3rds','3star','55','70','78','80s','86','87','99','_____'])
  
  #initialize TFIDF vectorizer
  vectorizer = TfidfVectorizer(stop_words=stop_words,ngram_range=(1,4))

  #create an empty list
  tfidf_vectorized_docs=[]

  #loop over the docs
  for doc in docs:
    X=vectorizer.fit_transform(doc['reviewText'])
    tfidf_vectorized_docs.append((vectorizer.get_feature_names(),X))
  
  return tfidf_vectorized_docs

tfidf_data=docs_TFIDF_vectorizer(docs)

tfidf_data_complete_list=[pd.DataFrame(tfidf_data[i][1].todense(),columns=tfidf_data[i][0]) for i in range(len(tfidf_data))]

selected_clusters

#The corresponding dataframes with tfidf data maybe called df_unclustered, df_0, df_1, df_2. We can easily assign names, as follows:
[df_unclustered,df_0, df_1, df_2]=[pd.DataFrame(tfidf_data[i][1].todense(),columns=tfidf_data[i][0]) for i in range(len(tfidf_data))]

#Let's take a look at couple example to check if this worked.
df_unclustered.head(3)

#List comprehension to create separate list for each cluster
n_prominent_words=[df.sum().nlargest(15).index for df in [df_unclustered,df_0, df_1, df_2]]

df_n_prominent_words=pd.DataFrame(n_prominent_words).T
df_n_prominent_words.columns=['df_unclustered','df_0', 'df_1', 'df_2']

def salient_terms(i):
  pyLDAvis.enable_notebook()

  from sklearn.feature_extraction.text import TfidfVectorizer

  stop_words = text.ENGLISH_STOP_WORDS.union(['00', '10', '100', '12', '15', '16', '20', '200', '24', '25',\
      '2nd', '30', '40', '45', '50', '60', '75', '80', '90','!',"''","'m","'s",',','.','...','He','I','It','My','Of','``',\
      '!',"''","'m","'re","'s",',','-','.','...','9','An','Ca','Do','I','It','S.','``','!',"''","'s",'(',')',',','-','.',\
      'b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y','&',"'ll",'-D','5',':','?',\
      "'", '0', '1', '2', '3', '4', '6', '7', '8', 'A', 'C', 'D', 'H', 'M', 'O', 'S', '`','#',"'ve",'*','--','..','....','10/10','4/5',';',\
      'As','At','HE','IS','IT','If','In','MY','No','ON','On','PR','SO','So','St','To','US','We','/', 'E', 'N', 'P', 'R', 'T', 'U', 'W', 'Y',\
      '$','%',"'S","'d",'.....','1/2','1/3','105','12-lead','125','14','150','198','1\\23\\18','22','221','27','3-3.5','35','AM','Be','By',\
      'CK','DJ','De','Dr','HM','JE','K.','L','MB','Mr','Ms','R.','TO','W.','YA','B', 'J', 'K', '\\','@','Im','Me','Is','000','100ish','11',\
      '178','191','1945','1st','2015','2016','260','36','360','3rds','3star','55','70','78','80s','86','87','99','_____'])
  vectorizer = TfidfVectorizer(stop_words=stop_words,ngram_range=(1,4),max_features=1000)
  X=vectorizer.fit_transform(df_n_prominent_words.iloc[:,i])
  lda=LatentDirichletAllocation(n_components=3,random_state=22)
  lda.fit(X)
  p=pyLDAvis.sklearn.prepare(lda,X,vectorizer,R=3)
  pyLDAvis.save_html(p,'/content/drive/My Drive/amazon_reviews_project/images/LDA_File_unclustered_group_24.html')
  return p

salient_terms(2)

